{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following can be loaded using the jupyter \"magic\" %load \n",
    "# %load ../helpers.py\n",
    "%matplotlib inline\n",
    "%env NI_PAGER=\"cat\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os.path import join, split, exists, splitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import box, shape\n",
    "from shapely.affinity import scale\n",
    "import shapely\n",
    "import fiona\n",
    "from pyproj import Proj, transform\n",
    "from fiona.crs import from_epsg\n",
    "from unidecode import unidecode\n",
    "import itertools\n",
    "import string\n",
    "\n",
    "ONE_DEGREE_LATITUDE_IN_KM = 111.0\n",
    "GEOCODING_DATA_FOLDER = \"./raw_data\"\n",
    "OUTPUT_DATA_FOLDER = \"./geohash_data\"\n",
    "COUNTRY_GH_PRECISION = 4\n",
    "DMA_GH_PRECISION = 4\n",
    "CITY_GH_PRECISION = 5\n",
    "REGION_GH_PRECISION = 4\n",
    "GEOHASH_ALPHABET='0123456789bcdefghjkmnpqrstuvwxyz'\n",
    "OK_CHARACTERS = set(string.letters + '_')\n",
    "NON_ARCTIC_WORLD = box(-180, -70, 180, 70)\n",
    "\n",
    "\n",
    "OUTPUT_FN_TEMPLATE = '{}_{}s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gh_bbox_size(size):\n",
    "    long_precision = np.ceil(size/2.0)\n",
    "    lat_precision = np.floor(size/2.0)\n",
    "#     print long_precision, lat_precision\n",
    "    long_step = 360.0/2**(long_precision)\n",
    "    lat_step = 180.0/2**(lat_precision)\n",
    "#     print long_step, lat_step\n",
    "    return long_step, lat_step\n",
    "\n",
    "\n",
    "def gh_bbox(point, size):\n",
    "    long_step, lat_step = gh_bbox_size(size)\n",
    "    point_long, point_lat = point\n",
    "    long_min, lat_min = np.floor(point_long/long_step)*long_step, np.floor(point_lat/lat_step)*lat_step\n",
    "    long_max, lat_max = np.ceil(point_long/long_step)*long_step, np.ceil(point_lat/lat_step)*lat_step\n",
    "    return long_min, lat_min, long_max, lat_max\n",
    "\n",
    "\n",
    "def make_grid(bounds, gh_size):\n",
    "    long_min, lat_min, long_max, lat_max = bounds\n",
    "    grid_top_left = gh_bbox((long_min, lat_min), gh_size)[:2]\n",
    "    grid_bottom_right = gh_bbox((long_max, lat_max), gh_size)[2:]\n",
    "    \n",
    "    long_step, lat_step = gh_bbox_size(gh_size)\n",
    "    longs = np.arange(grid_top_left[0], grid_bottom_right[0] + long_step, long_step)\n",
    "    lats = np.arange(grid_top_left[1], grid_bottom_right[1] + lat_step, lat_step)\n",
    "    long_pairs = zip(longs, longs[1:])\n",
    "    lat_pairs = zip(lats, lats[1:])\n",
    "    bounds = ((long_pair, lat_pair) for lat_pair, long_pair in itertools.product(lat_pairs, long_pairs))\n",
    "    bboxes = [(long_min, lat_min, long_max, lat_max) \n",
    "              for (long_min, long_max), (lat_min, lat_max) in bounds ]\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def search_gh_levels(search_space, precisions, geo_buffered=False):\n",
    "    output = []\n",
    "    this_precision = precisions[0]\n",
    "    if not search_space.bounds:\n",
    "        return []\n",
    "    ghs = make_grid(search_space.bounds, this_precision)\n",
    "    for box_coords in ghs:\n",
    "        coord_box = box(*box_coords)\n",
    "        intersection = coord_box.intersection(search_space)\n",
    "        centroid = tuple(coord_box.centroid.coords)[0]\n",
    "        if intersection.area == coord_box.area:\n",
    "            output.append((centroid, this_precision))\n",
    "        elif intersection.area == 0:\n",
    "            pass\n",
    "        else:\n",
    "            if len(precisions) > 1:\n",
    "                higher_precision_ghs = search_gh_levels(intersection, \n",
    "                                                        precisions[1:],\n",
    "                                                       geo_buffered)\n",
    "                output += higher_precision_ghs\n",
    "            elif not geo_buffered:  ## If there's no buffer, take an outer hull\n",
    "                output.append((centroid, this_precision))\n",
    "            else:  ## If the data has been buffered, only take an inner hull\n",
    "                pass\n",
    "    return output\n",
    "\n",
    "\n",
    "def shape_to_geohashes(input_geometry, gh_precision, buffer_km=0, steps=3):\n",
    "    if isinstance(input_geometry, shapely.geometry.Polygon):\n",
    "        input_shapes = [input_geometry]\n",
    "    else:\n",
    "        input_shapes = input_geometry.geoms\n",
    "        \n",
    "    if buffer_km != 0:\n",
    "        geo_buffer = True\n",
    "        input_shapes = [buffer_shape(s, buffer_km) for s in input_shapes]\n",
    "    else:\n",
    "        geo_buffer = False\n",
    "        \n",
    "\n",
    "    points_and_precisions = []\n",
    "    for g in input_shapes:\n",
    "#         if not g.is_valid: g = g.buffer(0)\n",
    "        assert g.is_valid\n",
    "        precisions = [(gh_precision-i)*5 for i in range(steps - 1, -1, -1)]\n",
    "        shape_point_precisions = search_gh_levels(g, precisions)\n",
    "        points_and_precisions += shape_point_precisions\n",
    "    return points_and_precisions\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Test code below. Check out gofreerange.geohash.com\n",
    "\"\"\"\n",
    "test_box_coords = (-11.25, 50.625, 0, 56.25)\n",
    "test_box = box(*test_box_coords)\n",
    "print test_box\n",
    "print test_box.bounds\n",
    "# grid = make_grid(test_box.bounds, 15)\n",
    "print gh_bbox((-1, 53), 10)\n",
    "print gh_bbox((-1, 53), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_process_properties(ps, _prop_names):\n",
    "    clean_ps = [unidecode(p) if isinstance(p, basestring) else str(p) for p in ps]\n",
    "    return clean_ps\n",
    "\n",
    "def default_process_shape(s):\n",
    "    return s\n",
    "\n",
    "def shapefile_gh_run(input_fn, output_fn, properties, process_shape, \n",
    "                     preprocess_shape=default_process_shape,\n",
    "                     process_properties=default_process_properties,\n",
    "                     process_shape_kwargs={}, hadoop=False):\n",
    "    if hadoop:\n",
    "        !ni hdfst://{input_fn} \\>input_file.tmp\n",
    "        input_fn = 'input_file.tmp'\n",
    "    count = 0\n",
    "    with open('{}.tmp'.format(output_fn), 'w') as output_f:\n",
    "        with fiona.open(input_fn, 'r') as source:\n",
    "            for f in source:\n",
    "                properties_dict = f['properties']\n",
    "                raw_shape_properties = [properties_dict[prop] for prop in properties]\n",
    "                if any(x is None for x in raw_shape_properties):\n",
    "                    print properties_dict\n",
    "                    print raw_shape_properties, \"NULLS\", \"\\n\\n\\n\\n\"\n",
    "                    continue\n",
    "                shape_properties = process_properties(raw_shape_properties, properties)\n",
    "                properties_str = '\\t'.join(shape_properties)\n",
    "                if count %10 == 0: print properties_str\n",
    "                \n",
    "                this_shape = shape(f['geometry'])\n",
    "                this_shape = preprocess_shape(this_shape)\n",
    "                this_shape = this_shape.buffer(0)\n",
    "                reasonable_shape = NON_ARCTIC_WORLD.intersection(this_shape)\n",
    "                shape_data = process_shape(reasonable_shape, **process_shape_kwargs)\n",
    "                if count % 10 == 0: print properties_str, len(shape_data)\n",
    "                for shape_datum_str in shape_data:\n",
    "                    output_f.write('{}\\t{}\\n'.format(shape_datum_str, properties_str))\n",
    "                count += 1\n",
    "    if 'precision' in process_shape_kwargs:\n",
    "        postprocess_gh_data('{}.tmp'.format(output_fn), output_fn, \n",
    "                           process_shape_kwargs['precision'])\n",
    "    !ni {output_fn}.tmp gABu \\>{output_fn}\n",
    "    return\n",
    "              \n",
    "\n",
    "def explore_shapefile(input_fn, n=3):\n",
    "    with fiona.open(input_fn, 'r') as source:\n",
    "        count = 0\n",
    "        for f in source:\n",
    "            properties_dict = f['properties']\n",
    "            print properties_dict\n",
    "            count += 1\n",
    "            this_shape = shape(f['geometry'])\n",
    "            print source.crs\n",
    "            print f.crs\n",
    "#             print this_shape\n",
    "            if count > n: break\n",
    "\n",
    "def geohash_cover(input_shape, precision, buffer_km=0):\n",
    "    point_precisions = shape_to_geohashes(input_shape, precision, \n",
    "                                          buffer_km, steps=2)\n",
    "    output_strs = ['{}\\t{}\\t{}'.format(lat, lon, precision) \n",
    "                   for (lon, lat), precision in point_precisions]\n",
    "    return output_strs\n",
    "\n",
    "\n",
    "def generate_centroid(input_shape):\n",
    "    centroid = input_shape.centroid\n",
    "    return ['{}\\t{}'.format(centroid.y, centroid.x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_gh_data(input_fn, output_fn, gh_precision):\n",
    "    !ni {input_fn} p'r ghe(a, b, c/5), FR 3' \\>intermediate_gh.tmp\n",
    "    with open('{}.tmp'.format(output_fn), 'w') as output_f:\n",
    "        for line in open('intermediate_gh.tmp', 'r'):\n",
    "            raw_gh = line.split('\\t')[0]\n",
    "            line_data = '\\t'.join(line.split('\\t')[1:])\n",
    "            append_ghs = [GEOHASH_ALPHABET] * (gh_precision - len(raw_gh))\n",
    "            output_ghs = itertools.product([raw_gh], *append_ghs)\n",
    "            for output_gh in output_ghs:\n",
    "                output_f.write('{}\\t{}'.format(''.join(output_gh), line_data))                      \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Downloaded from: https://github.com/simzou/nielsen-dma/blob/master/nielsentopo.json\n",
    "Then converted from topojson to shapefile.\n",
    "\"\"\"\n",
    "\n",
    "def process_dma_properties(ps, prop_names):\n",
    "    return [process_dma_property(p, prop_name) for p, prop_name in zip(ps, prop_names)]\n",
    "\n",
    "def process_dma_property(p, prop_name):\n",
    "    fmt_str = 'geo__{}__{}'\n",
    "    if prop_name == 'dma':\n",
    "        return fmt_str.format('dma_id', p)\n",
    "    else:\n",
    "        while '(' in p:\n",
    "            p = p[:p.index('(') -1] + p[p.index(')') + 1:]\n",
    "        dma_pieces = p.split(', ')\n",
    "        dma, state = dma_pieces[0], dma_pieces[-1]\n",
    "        state = state[:2]\n",
    "        short_dma = dma.split('-')[0]\n",
    "        short_dma = short_dma.replace(' ', '_')\n",
    "        short_dma = ''.join(ch for ch in short_dma if ch in OK_CHARACTERS)\n",
    "        return fmt_str.format('dma', '{}_{}'.format(short_dma, state))\n",
    "\n",
    "dma_output_fn = join(OUTPUT_DATA_FOLDER,\n",
    "                     OUTPUT_FN_TEMPLATE.format('dma', 'gh{}'.format(DMA_GH_PRECISION)))\n",
    "dma_input_fn = join(GEOCODING_DATA_FOLDER, 'nielsen_dma/nielsen_dma.shp')\n",
    "dma_properties = ['dma1', 'dma']\n",
    "dma_kwargs = {'precision': DMA_GH_PRECISION}\n",
    "\n",
    "shapefile_gh_run(input_fn=dma_input_fn, \n",
    "                 output_fn=dma_output_fn, \n",
    "                 properties=dma_properties, \n",
    "                 process_shape=geohash_cover, \n",
    "                 process_properties=process_dma_properties,\n",
    "                 process_shape_kwargs=dma_kwargs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
